{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ewenmichel/miniconda3/envs/altegrad/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from random import shuffle, choices\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    balanced_accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    "    hamming_loss,\n",
    ")\n",
    "\n",
    "from GRU_pipeline import DataHolder, paths, MultiModalDicDataset, GRUModel, pad_collate, eval_on_val, train_one_epoch, BertClassif\n",
    "from models import GRUMultiModal, GRUBiModal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DH = DataHolder(**paths, none_as_class=True)\n",
    "\n",
    "train_test_1 = DH.stratified_train_test_split(feature = 'multimodal', speaker = 1, none_count = 2000, test_size = .15, val_size = .17)\n",
    "train_test_2 = DH.stratified_train_test_split(feature = 'multimodal', speaker = 2, none_count = 2000, test_size = .15, val_size = .17)\n",
    "\n",
    "class_weights = train_test_1['class_weights']\n",
    "class_weights = torch.Tensor([(1 - x) ** 3 for x in class_weights])\n",
    "\n",
    "train_dataset_1 = MultiModalDicDataset(**train_test_1['data'])\n",
    "test_dataset_1 = MultiModalDicDataset(train_dic_openface = train_test_1[\"data\"][\"test_dic_openface\"], train_dic_opensmile = train_test_1[\"data\"][\"test_dic_opensmile\"], features_openface = train_test_1[\"data\"][\"features_openface\"], features_opensmile = train_test_1[\"data\"][\"features_opensmile\"], embeds = train_test_1[\"data\"][\"embeds\"], targets = train_test_1[\"data\"][\"targets\"], test_dic_openface = None, test_dic_opensmile = None)\n",
    "val_dataset_1 = MultiModalDicDataset(train_dic_openface = train_test_1[\"data\"][\"valid_dic_openface\"], train_dic_opensmile = train_test_1[\"data\"][\"valid_dic_opensmile\"], features_openface = train_test_1[\"data\"][\"features_openface\"], features_opensmile = train_test_1[\"data\"][\"features_opensmile\"], embeds = train_test_1[\"data\"][\"embeds\"], targets = train_test_1[\"data\"][\"targets\"], test_dic_openface = None, test_dic_opensmile = None)\n",
    "\n",
    "train_loader_1 = DataLoader(train_dataset_1, batch_size = 200, shuffle = True, collate_fn = pad_collate)\n",
    "val_loader_1 = DataLoader(val_dataset_1, batch_size = 200, shuffle = True, collate_fn = pad_collate)\n",
    "test_loader_1 = DataLoader(test_dataset_1, batch_size = 200, shuffle = True, collate_fn = pad_collate)\n",
    "\n",
    "train_dataset_2 = MultiModalDicDataset(**train_test_2['data'])\n",
    "test_dataset_2 = MultiModalDicDataset(train_dic_openface = train_test_2[\"data\"][\"test_dic_openface\"], train_dic_opensmile = train_test_2[\"data\"][\"test_dic_opensmile\"], features_openface = train_test_2[\"data\"][\"features_openface\"], features_opensmile = train_test_2[\"data\"][\"features_opensmile\"], embeds = train_test_2[\"data\"][\"embeds\"], targets = train_test_2[\"data\"][\"targets\"], test_dic_openface = None, test_dic_opensmile = None)\n",
    "val_dataset_2 = MultiModalDicDataset(train_dic_openface = train_test_2[\"data\"][\"valid_dic_openface\"], train_dic_opensmile = train_test_2[\"data\"][\"valid_dic_opensmile\"], features_openface = train_test_2[\"data\"][\"features_openface\"], features_opensmile = train_test_2[\"data\"][\"features_opensmile\"], embeds = train_test_2[\"data\"][\"embeds\"], targets = train_test_2[\"data\"][\"targets\"], test_dic_openface = None, test_dic_opensmile = None)\n",
    "\n",
    "train_loader_2 = DataLoader(train_dataset_2, batch_size = 200, shuffle = True, collate_fn = pad_collate)\n",
    "val_loader_2 = DataLoader(val_dataset_2, batch_size = 200, shuffle = True, collate_fn = pad_collate)\n",
    "test_loader_2 = DataLoader(test_dataset_2, batch_size = 200, shuffle = True, collate_fn = pad_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/500 [00:00<06:10,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "================\n",
      "Training epoch 0 :\n",
      "Train loss = 2.4325, Val loss = 2.2748\n",
      "================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 30/500 [00:09<02:25,  3.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 31:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 31/500 [00:10<02:24,  3.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 31:\n",
      "================\n",
      "Training epoch 30 :\n",
      "Train loss = 0.4184, Val loss = 0.4590\n",
      "================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 60/500 [00:18<02:09,  3.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 61:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 61/500 [00:19<02:11,  3.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 61:\n",
      "================\n",
      "Training epoch 60 :\n",
      "Train loss = 0.2715, Val loss = 0.3359\n",
      "================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 90/500 [00:27<02:11,  3.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 91:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 91/500 [00:28<02:07,  3.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 91:\n",
      "================\n",
      "Training epoch 90 :\n",
      "Train loss = 0.1675, Val loss = 0.2253\n",
      "================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 120/500 [00:37<01:57,  3.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 121:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 121/500 [00:37<01:57,  3.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 121:\n",
      "================\n",
      "Training epoch 120 :\n",
      "Train loss = 0.1262, Val loss = 0.1986\n",
      "================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 150/500 [00:46<01:45,  3.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 151:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 151/500 [00:46<01:46,  3.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 151:\n",
      "================\n",
      "Training epoch 150 :\n",
      "Train loss = 0.1118, Val loss = 0.1965\n",
      "================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 180/500 [00:57<02:44,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 181:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 181/500 [00:58<02:23,  2.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 181:\n",
      "================\n",
      "Training epoch 180 :\n",
      "Train loss = 0.0960, Val loss = 0.2065\n",
      "================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 210/500 [01:06<01:25,  3.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 211:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 211/500 [01:07<01:25,  3.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 211:\n",
      "================\n",
      "Training epoch 210 :\n",
      "Train loss = 0.0804, Val loss = 0.2212\n",
      "================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 240/500 [01:18<01:23,  3.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 241:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 241/500 [01:18<01:26,  3.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 241:\n",
      "================\n",
      "Training epoch 240 :\n",
      "Train loss = 0.0702, Val loss = 0.2267\n",
      "================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 270/500 [01:28<01:12,  3.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 271:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 271/500 [01:28<01:10,  3.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 271:\n",
      "================\n",
      "Training epoch 270 :\n",
      "Train loss = 0.0534, Val loss = 0.2401\n",
      "================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 300/500 [01:37<01:00,  3.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 301:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 300/500 [01:37<01:05,  3.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 301:\n",
      "================\n",
      "Training epoch 300 :\n",
      "Train loss = 0.0456, Val loss = 0.2531\n",
      "================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "multi_modal_params = {\"embeddings_dim\" : 768, \n",
    "            \"audio_input_dim\":23, \n",
    "            \"audio_hidden_dim\":32, \n",
    "            \"audio_layer_dim\":3, \n",
    "            \"video_input_dim\":17, \n",
    "            \"video_hidden_dim\":32, \n",
    "            \"video_layer_dim\":3, \n",
    "            \"output_dim\":6, \n",
    "            \"dropout_prob\":.1}\n",
    "AUDIOPARAMS = {\"embeddings_dim\" : 768, \"input_dim\":23, \"hidden_dim\":32, \"layer_dim\":3, \"output_dim\":6, \"dropout_prob\":.1}\n",
    "VIDEOPARAMS = {\"embeddings_dim\" : 768, \"input_dim\":17, \"hidden_dim\":32, \"layer_dim\":3, \"output_dim\":6, \"dropout_prob\":.1}\n",
    "\n",
    "multi_modal_model = GRUMultiModal(**multi_modal_params)\n",
    "audio_model = GRUBiModal(**AUDIOPARAMS)\n",
    "video_model = GRUBiModal(**VIDEOPARAMS)\n",
    "txt_model = BertClassif()\n",
    "\n",
    "criterion = torch.nn.BCELoss(class_weights)\n",
    "optimizer = torch.optim.Adam(params = txt_model.parameters(), lr = 5e-5)\n",
    "\n",
    "EPOCHS = 500\n",
    "\n",
    "hist_train_loss = []\n",
    "hist_val_loss = []\n",
    "device = torch.device(\"mps\")\n",
    "stagnation= 0\n",
    "best_vloss = 1000\n",
    "for epoch in tqdm(range(EPOCHS), total = EPOCHS):\n",
    "\n",
    "    curr_train_loss, curr_val_loss = hist_train_loss, hist_val_loss\n",
    "    if epoch % 30 == 0:\n",
    "        print(\"EPOCH {}:\".format(epoch + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    txt_model.train(True)\n",
    "    hist_train_loss, hist_val_loss, stagnation, best_vloss = train_one_epoch(\n",
    "        epoch, txt_model, criterion, train_loader_1, train_loader_2, val_loader_1, val_loader_2, hist_train_loss, hist_val_loss, stagnation, best_vloss, opt = optimizer\n",
    "    )\n",
    "\n",
    "    if stagnation > 4:\n",
    "        break\n",
    "hist_train_loss = [x.detach().numpy() for x in hist_train_loss]\n",
    "hist_val_loss = [x.detach().numpy() for x in hist_val_loss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from GRU_pipeline import BertClassif\n",
    "\n",
    "multi_modal_params = {\"embeddings_dim\" : 768, \n",
    "            \"audio_input_dim\":23, \n",
    "            \"audio_hidden_dim\":32, \n",
    "            \"audio_layer_dim\":3, \n",
    "            \"video_input_dim\":17, \n",
    "            \"video_hidden_dim\":32, \n",
    "            \"video_layer_dim\":3, \n",
    "            \"output_dim\":6, \n",
    "            \"dropout_prob\":.1}\n",
    "AUDIOPARAMS = {\"embeddings_dim\" : 768, \"input_dim\":23, \"hidden_dim\":32, \"layer_dim\":3, \"output_dim\":6, \"dropout_prob\":.1}\n",
    "VIDEOPARAMS = {\"embeddings_dim\" : 768, \"input_dim\":17, \"hidden_dim\":32, \"layer_dim\":3, \"output_dim\":6, \"dropout_prob\":.1}\n",
    "\n",
    "\n",
    "multi_modal_model = GRUMultiModal(**multi_modal_params)\n",
    "audio_model = GRUBiModal(**AUDIOPARAMS)\n",
    "video_model = GRUBiModal(**VIDEOPARAMS)\n",
    "txt_model = BertClassif()\n",
    "\n",
    "multi_modal_model.load_state_dict(torch.load(\"models/MultiModalBert\"))\n",
    "video_model.load_state_dict(torch.load(\"models/BertvideoBimodal\"))\n",
    "audio_model.load_state_dict(torch.load(\"models/BertaudioBimodal\"))\n",
    "txt_model.load_state_dict(torch.load(\"models/BertClassif\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00,  6.83it/s]\n",
      "2it [00:00,  8.27it/s]\n"
     ]
    }
   ],
   "source": [
    "fin_targets = []\n",
    "txt_fin_outputs = []\n",
    "audio_fin_outputs = []\n",
    "video_fin_outputs = []\n",
    "multimodal_fin_outputs = []\n",
    "\n",
    "for j, batch in tqdm(enumerate(test_loader_1)):\n",
    "    features_of, features_os, embeds, targets = batch['features_of'], batch['features_os'], batch['embeds'], batch['targets']\n",
    "    text_pred = txt_model(embeds)\n",
    "    audio_pred = audio_model(embeds, features_os)\n",
    "    video_pred = video_model(embeds, features_of)\n",
    "    multimodal_pred = multi_modal_model(embeds, features_os, features_of)\n",
    "    fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "    txt_fin_outputs.extend(np.array(text_pred.cpu().detach().numpy()).tolist())\n",
    "    audio_fin_outputs.extend(np.array(audio_pred.cpu().detach().numpy()).tolist())\n",
    "    video_fin_outputs.extend(np.array(video_pred.cpu().detach().numpy()).tolist())\n",
    "    multimodal_fin_outputs.extend(np.array(multimodal_pred.cpu().detach().numpy()).tolist())\n",
    "\n",
    "for j, batch in tqdm(enumerate(test_loader_2)):\n",
    "    features_of, features_os, embeds, targets = batch['features_of'], batch['features_os'], batch['embeds'], batch['targets']\n",
    "    text_pred = txt_model(embeds)\n",
    "    audio_pred = audio_model(embeds, features_os)\n",
    "    video_pred = video_model(embeds, features_of)\n",
    "    multimodal_pred = multi_modal_model(embeds, features_os, features_of)\n",
    "    fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "    txt_fin_outputs.extend(np.array(text_pred.cpu().detach().numpy()).tolist())\n",
    "    audio_fin_outputs.extend(np.array(audio_pred.cpu().detach().numpy()).tolist())\n",
    "    video_fin_outputs.extend(np.array(video_pred.cpu().detach().numpy()).tolist())\n",
    "    multimodal_fin_outputs.extend(np.array(multimodal_pred.cpu().detach().numpy()).tolist())\n",
    "    \n",
    "y_true = np.asarray(fin_targets)\n",
    "txt_pred_n = np.asarray(txt_fin_outputs)\n",
    "audio_pred_n = np.asarray(audio_fin_outputs)\n",
    "video_pred_n = np.asarray(video_fin_outputs)\n",
    "multimodal_pred_n = np.asarray(multimodal_fin_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9820448748609608 0.986964227761618 0.9880406268928525 0.8622884027926877\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve, classification_report\n",
    "\n",
    "# auc scores\n",
    "auc_score1 = roc_auc_score(y_true, txt_pred_n)\n",
    "auc_score2 = roc_auc_score(y_true, audio_pred_n)\n",
    "auc_score3 = roc_auc_score(y_true, video_pred_n)\n",
    "auc_score4 = roc_auc_score(y_true, multimodal_pred_n)\n",
    "\n",
    "print(auc_score1, auc_score2, auc_score3, auc_score4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          SD       0.80      0.81      0.80        63\n",
      "          QE       0.92      0.79      0.85        14\n",
      "          SV       0.00      0.00      0.00         8\n",
      "          PR       1.00      0.25      0.40        12\n",
      "          HD       0.80      0.76      0.78        62\n",
      "        None       1.00      1.00      1.00       601\n",
      "\n",
      "   micro avg       0.96      0.94      0.95       760\n",
      "   macro avg       0.75      0.60      0.64       760\n",
      "weighted avg       0.95      0.94      0.94       760\n",
      " samples avg       0.95      0.95      0.94       760\n",
      "\n",
      "audio\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          SD       0.84      0.86      0.85        63\n",
      "          QE       0.93      0.93      0.93        14\n",
      "          SV       0.00      0.00      0.00         8\n",
      "          PR       1.00      0.58      0.74        12\n",
      "          HD       0.80      0.85      0.83        62\n",
      "        None       1.00      1.00      1.00       601\n",
      "\n",
      "   micro avg       0.97      0.96      0.96       760\n",
      "   macro avg       0.76      0.70      0.72       760\n",
      "weighted avg       0.96      0.96      0.96       760\n",
      " samples avg       0.96      0.96      0.96       760\n",
      "\n",
      "video\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          SD       0.88      0.90      0.89        63\n",
      "          QE       0.93      0.93      0.93        14\n",
      "          SV       0.00      0.00      0.00         8\n",
      "          PR       1.00      0.58      0.74        12\n",
      "          HD       0.82      0.89      0.85        62\n",
      "        None       1.00      1.00      1.00       601\n",
      "\n",
      "   micro avg       0.97      0.96      0.97       760\n",
      "   macro avg       0.77      0.72      0.73       760\n",
      "weighted avg       0.96      0.96      0.96       760\n",
      " samples avg       0.97      0.97      0.97       760\n",
      "\n",
      "multimodal\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          SD       0.75      0.14      0.24        63\n",
      "          QE       0.00      0.00      0.00        14\n",
      "          SV       0.00      0.00      0.00         8\n",
      "          PR       0.00      0.00      0.00        12\n",
      "          HD       0.00      0.00      0.00        62\n",
      "        None       0.80      1.00      0.89       601\n",
      "\n",
      "   micro avg       0.80      0.80      0.80       760\n",
      "   macro avg       0.26      0.19      0.19       760\n",
      "weighted avg       0.70      0.80      0.72       760\n",
      " samples avg       0.81      0.81      0.81       760\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ewenmichel/miniconda3/envs/altegrad/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/ewenmichel/miniconda3/envs/altegrad/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/ewenmichel/miniconda3/envs/altegrad/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/ewenmichel/miniconda3/envs/altegrad/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/ewenmichel/miniconda3/envs/altegrad/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/ewenmichel/miniconda3/envs/altegrad/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/ewenmichel/miniconda3/envs/altegrad/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "results = [txt_pred_n, audio_pred_n, video_pred_n, multimodal_pred_n]\n",
    "names = [\"text\", \"audio\", \"video\", \"multimodal\"]\n",
    "\n",
    "for pred, n in zip(results, names):\n",
    "    pred_final = np.where(pred > .5, 1, 0)\n",
    "    print(n)\n",
    "    print(classification_report(y_true, pred_final, target_names = ['SD', 'QE', 'SV', 'PR', 'HD', \"None\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0b361c001a35e4df5d61a1c2d55742e80469c144a59b264207a8ccea62793ab6"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 ('altegrad')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
